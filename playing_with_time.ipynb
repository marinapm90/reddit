{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In which subreddit do you want to post? (example: https://www.reddit.com/r/learnpython/ --> just type learnpython)\n",
      "marinamola\n"
     ]
    }
   ],
   "source": [
    "print('In which subreddit do you want to post? (example: https://www.reddit.com/r/learnpython/ --> just type learnpython)')\n",
    "subreddit = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting posts from https://reddit.com/r/marinamola/.json?limit=100...\n",
      "C'mon guy! try something real\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from functools import lru_cache\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "try: \n",
    "    url = \"https://reddit.com/r/{}/.json?limit=100\".format(subreddit)\n",
    "\n",
    "    headers = {'User-Agent': \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:66.0) Gecko/20100101 Firefox/66.0\"}\n",
    "    REDDIT_ROOT_URL = \"https://reddit.com\"\n",
    "\n",
    "    def get_with_headers(url):\n",
    "        return requests.get(url, headers=headers)\n",
    "\n",
    "    @lru_cache(maxsize=32)\n",
    "    def get_subreddit_posts(subreddit_url):\n",
    "        print(f\"Getting posts from {url}...\")\n",
    "        response = get_with_headers(url)\n",
    "        raw_posts = response.json()['data']['children']\n",
    "\n",
    "        posts = []\n",
    "        for raw_post in raw_posts:\n",
    "            post = {}\n",
    "            raw_post = raw_post['data']\n",
    "            post['name'] = raw_post['name']\n",
    "            post['title'] = raw_post['title']\n",
    "            post['score'] = raw_post['score']\n",
    "            post['url'] = REDDIT_ROOT_URL + raw_post['permalink']\n",
    "            post['created_utc'] = raw_post['created_utc']\n",
    "            post['num_comments'] = raw_post['num_comments']\n",
    "\n",
    "            posts.append(post)\n",
    "\n",
    "        return posts\n",
    "\n",
    "    # Getting all posts.\n",
    "\n",
    "    posts = get_subreddit_posts(url)\n",
    "    all_posts= []\n",
    "\n",
    "    for post in posts:\n",
    "        try:\n",
    "            posts = get_subreddit_posts(url)\n",
    "            last_value = [ sub['name'] for sub in posts][-1]\n",
    "            url = \"{}&after={}\".format(url,last_value)\n",
    "            all_posts += get_subreddit_posts(url)\n",
    "            all_posts\n",
    "\n",
    "        except (IndexError):\n",
    "            pass\n",
    "\n",
    "    # Creating data frame\n",
    "\n",
    "    all_posts = pd.DataFrame(all_posts)\n",
    "\n",
    "    all_posts[\"created_utc\"] = all_posts[\"created_utc\"].apply(datetime.fromtimestamp)\n",
    "    all_posts.rename(columns={'created_utc':'date_hour'},inplace=True)\n",
    "\n",
    "    # Splitting the column into date and time.\n",
    "\n",
    "    all_posts['date'] = [d.date() for d in all_posts['date_hour']]\n",
    "    all_posts['time'] = [d.time() for d in all_posts['date_hour']]\n",
    "\n",
    "    # Splitting the column into hour, minutes and seconds.\n",
    "\n",
    "    all_posts['hour'] = all_posts['date_hour'].dt.hour\n",
    "    all_posts['minute'] = all_posts['date_hour'].dt.minute\n",
    "    all_posts['second'] = all_posts['date_hour'].dt.second\n",
    "\n",
    "    # Adding weekday.\n",
    "\n",
    "    all_posts['weekday'] = [d.weekday() for d in all_posts['date']]\n",
    "\n",
    "\n",
    "    # Creating a copy with only the important columns.\n",
    "\n",
    "    all_posts_score = all_posts[['date', 'score','weekday']].copy()\n",
    "\n",
    "    # Some weekdays are repeated several times.\n",
    "\n",
    "    unique = all_posts_score.date.unique().tolist()\n",
    "    unique_df = pd.DataFrame(unique,columns =['date'])\n",
    "    unique_df['weekday'] = [d.weekday() for d in unique_df['date']]\n",
    "    repeated = unique_df.weekday.value_counts()\n",
    "    repeated = repeated.to_dict()\n",
    "\n",
    "    # Creating a data frame with the number of times a weekday is repeated.\n",
    "\n",
    "    repeated_df = pd.DataFrame.from_dict(repeated, orient='index')\n",
    "\n",
    "    # Creating a data frame with the sum of total score per weekday.\n",
    "\n",
    "    score_per_day = all_posts_score.groupby(by='weekday').agg({'score':['sum']})\n",
    "    posts_per_day = all_posts_score.weekday.value_counts().to_frame()\n",
    "\n",
    "    # Concatenate both data frames into one.\n",
    "\n",
    "    result = pd.concat([posts_per_day, repeated_df,score_per_day], axis=1, sort=False)\n",
    "    result.rename(columns ={0:'repeated_weekday'},inplace=True)\n",
    "\n",
    "\n",
    "    # Divide the columns \"weekday\" and \"score\" by \"repeated_weekday\" to have the number of posts and the score per day.\n",
    "\n",
    "    result['posts_per_day']= result.weekday/result.repeated_weekday\n",
    "    result['score_per_day']= result['score', 'sum']/result.repeated_weekday\n",
    "    result = result.sort_values(['posts_per_day'], ascending = False)\n",
    "    result.reset_index(inplace=True)\n",
    "\n",
    "    # Replacing numbers per days.\n",
    "\n",
    "    result['index'] = result['index'].replace(0,'Monday').replace(1,'Tuesday').replace(2,'Wednesday').replace(3,'Thursday').replace(4,'Friday').replace(5,'Saturday').replace(6,'Sunday')\n",
    "    result.rename = result.rename(columns = {'index':'day'}, inplace = True)\n",
    "\n",
    "\n",
    "    # Dropping useless columns.\n",
    "\n",
    "    result.drop(['weekday', 'repeated_weekday', ('score', 'sum')], axis=1,inplace=True)\n",
    "\n",
    "    result_score = result.sort_values(['score_per_day'], ascending = False)\n",
    "    result_score.reset_index(inplace=True)\n",
    "    highest_score = result_score.day[0]\n",
    "\n",
    "    # Grouping a dataset by weekday, hour and total score.\n",
    "\n",
    "    hours_weekday = all_posts.groupby(by=['weekday','hour']).agg({'score':['sum']}).reset_index()\n",
    "    hours_weekday['total_score'] = hours_weekday['score', 'sum']*1\n",
    "    hours_weekday.drop([('score', 'sum')], axis=1,inplace=True)\n",
    "    hours_weekday.sort_values(['total_score'],ascending= False, inplace=True)\n",
    "    hours_weekday['weekday'] = hours_weekday['weekday'].replace(0,'Monday').replace(1,'Tuesday').replace(2,'Wednesday').replace(3,'Thursday').replace(4,'Friday').replace(5,'Saturday').replace(6,'Sunday')\n",
    "\n",
    "    # Getting the best hour depending on the best day.\n",
    "\n",
    "    best_hour = hours_weekday[(hours_weekday['weekday']== highest_score)]\n",
    "    best_hour.sort_values(['total_score'],ascending= False,inplace=True)\n",
    "    time = best_hour.hour.iloc[0]\n",
    "\n",
    "    # Getting results\n",
    "\n",
    "    print(\"===========================================================\")\n",
    "    print(result)\n",
    "    print(\"===========================================================\")\n",
    "    print(result.day[0] + \" is the day that most posts are published.\")\n",
    "    print(\"Posts publised on {} are those with highest scores.\".format(highest_score))\n",
    "    print('The best time to post on {} is at {}H'.format(highest_score,time))\n",
    "\n",
    "except (KeyError):\n",
    "    print(\"C'mon guy! try something real!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
